# 第三章 实战环境搭建

- Spark源码编译
- Spark环境搭建
- Spark简单使用



## Spark源码编译

官网：http://spark.apache.org/docs/latest/building-spark.html

**前置要求：**
1）Building Spark using Maven requires Maven 3.3.9 or newer and Java 7+
2）export MAVEN_OPTS="-Xmx2g -XX:ReservedCodeCacheSize=512m"

**mvn编译命令：**

```shell
./build/mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -DskipTests clean package
```

前提：需要对maven有一定的了解(pom.xml)

```shell
<properties>

    <hadoop.version>2.2.0</hadoop.version>

    <protobuf.version>2.5.0</protobuf.version>

    <yarn.version>${hadoop.version}</yarn.version>

</properties>

<profile>

  <id>hadoop-2.6</id>

  <properties>

    <hadoop.version>2.6.4</hadoop.version>

    <jets3t.version>0.9.3</jets3t.version>

    <zookeeper.version>3.4.6</zookeeper.version>

    <curator.version>2.6.0</curator.version>

  </properties>

</profile>

./build/mvn -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0-cdh5.7.0 -DskipTests clean package
```



**推荐使用**

```shell
./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz  -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0-cdh5.7.0
```



**编译完成后：**
spark-$VERSION-bin-$NAME.tgz

spark-2.1.0-bin-2.6.0-cdh5.7.0.tgz



## Spark源码编译中的坑

```shell
./dev/make-distribution.sh  \
--name 2.6.0-cdh5.7.0  \
--tgz   \
-Pyarn  \
-Phadoop-2.6  \
-Phive  \
-Phive-thriftserver  \
-Dhadoop.version=2.6.0-cdh5.7.0
```

如果在编译过程中，你看到的异常信息不是太明显/看不懂，编译命令后 -X，就能看到更详细的编译信息

![遇到的坑](https://upload-images.jianshu.io/upload_images/5959612-fa04c730a026f689.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![遇到的坑](https://upload-images.jianshu.io/upload_images/5959612-963684003dbb2215.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



## Spark环境搭建

- local模式

启动错误

```shell
//启动
$park-shell --master local[2]

org.datanucleus.exceptions.NucleusException: Attempt to invoke the "BONECP" plugin to create a ConnectionPool gave an error : The specified datastore driver ("com.mysql.jdbc.Driver") was not found in the CLASSPATH. Please check your CLASSPATH specification, and the name of the driver.

解决：
将mysql-connector-java-5.1.27-bin.jar放入spark的jars包中
```



- Standalone模式

 Spark Standalone模式的架构和Hadoop HDFS/YARN很类似的

1 master + n worker

spark-env.sh
SPARK_MASTER_HOST=hadoop001
SPARK_WORKER_CORES=2
SPARK_WORKER_MEMORY=2g
SPARK_WORKER_INSTANCES=1

hadoop1 : master
hadoop2 : worker
hadoop3 : worker
hadoop4 : worker
...
hadoop10 : worker

slaves:
hadoop2
hadoop3
hadoop4
....
hadoop10

==> start-all.sh   会在 hadoop1机器上启动master进程，在slaves文件配置的所有hostname的机器上启动worker进程

```shell
//启动
$spark-shell --master spark://hadoop001:7077
```



**Spark WordCount统计**
val file = spark.sparkContext.textFile("file:///home/hadoop/data/wc.txt")
val wordCounts = file.flatMap(line => line.split(",")).map((word => (word, 1))).reduceByKey(_ + _)
wordCounts.collect



Boy-20180531
