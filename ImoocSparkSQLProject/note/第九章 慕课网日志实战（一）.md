# 第九章 慕课网日志实战（一）

[TOC]

## 1 慕课网日志分析实战项目

- 用户行为日志概述
- 离线数据处理架构
- 项目需求
- 功能实现
- Spark on YARN
- 性能调优



## 2 用户行为日志概述

为什么要记录用户访问行为日志？

- 网站页面的访问量
- 网站的黏性
- 推荐



**用户行为日志：**用户每次访问网站时所有的行为数据（访问、浏览、搜索、点击...）
	                   用户行为轨迹、流量日志



用户行为日志**生成渠道**

- Nginx
- Ajax

用户行为**日志内容**

![用户行为日志内容](https://upload-images.jianshu.io/upload_images/5959612-63bde947a311d300.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

日志数据内容：
1）访问的系统属性： 操作系统、浏览器等等
2）访问特征：点击的url、从哪个url跳转过来的(referer)、页面上的停留时间等
3）访问信息：session_id、访问ip(访问城市)等

例子：

2013-05-19 13:00:00     http://www.taobao.com/17/?tracker_u=1624169&type=1      B58W48U4WKZCJ5D1T3Z9ZY88RU7QA7B1        http://hao.360.cn/      1.196.34.243   



**用户行为日志分析的意义？**

- 网站的眼睛
- 网站的神经
- 网站的大脑



## 3 离线数据处理架构

**数据处理流程**
1）数据**采集**
	Flume： web日志写入到HDFS

2）数据**清洗**
	脏数据
	Spark、Hive、MapReduce 或者是其他的一些分布式计算框架 
	清洗完之后的数据可以存放在HDFS(Hive/Spark SQL)

3）数据**处理**
	按照我们的需要进行相应业务的统计和分析
	Spark、Hive、MapReduce 或者是其他的一些分布式计算框架

4）处理**结果入库**
	结果可以存放到RDBMS、NoSQL

5）数据的**可视化**
	通过图形化展示的方式展现出来：饼图、柱状图、地图、折线图
	ECharts、HUE、Zeppelin

离线数据处理架构

![离线数据处理架构](https://upload-images.jianshu.io/upload_images/5959612-2c1b861c7f693009.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



## 4 项目需求

- 需求一：统计**imooc主站**最受欢迎的课程/手记的Top N访问次数

- 需求二：按**地市统计**imooc主站最受欢迎的Top N课程

  1）根据IP地址提取出城市信息

  2）窗口函数在Spark SQL中的使用

- 需求三：按**流量统计**imooc主站最受欢迎的TonN课程



## 5 imooc网主站日志内容构成

imooc网主站日志介绍

- 访问时间
- 访问URL
- 访问过程耗费流量
- 访问IP地址



## 6 数据清洗之第一步原始日志解析

数据清洗

- 第一阶段将需要的字段从原始的日志中提取出来
- 然后根据提取的日志进行精细化操作

第一步清洗：抽取出我们需要的指定列的数据

```scala
package com.imooc.spark

import org.apache.spark.sql.SparkSession

/**
  * 第一步清洗：抽取出我们需要的指定列的数据
  */
object SparkStatFormatJob {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession.builder().appName("SparkStatFormatJob").master("local[2]").getOrCreate()

    val access = spark.sparkContext.textFile("src/main/data/10000_access.log")

    access.map(line => {
      val splits = line.split(" ")
      val ip = splits(0)

      /**
        * 原始日志的第三个个第四个字段拼接起来就是完整的访问时间：
        * [10/Nov/2016:00:01:02 +0800] ===> yyyy-MM-dd HH:mm:ss
        */
      val time = splits(3) + " " + splits(4)
      val url = splits(11).replace("\"", "")
      val traffic = splits(9)

      //(ip, DateUtils.parse(time), url, traffic)
      DateUtils.parse(time) + "\t" + url + "\t" + traffic + "\t" + ip
    }).saveAsTextFile("src/main/data/output/")
    //spark的saveAsTextFile方法只能指定文件夹，但是保存到本地的话，会报空指针错误。
    //参考：https://blog.csdn.net/kimyoungvon/article/details/51308651
    spark.stop()
  }
}
```

```scala
//日期时间解析工具类
import java.util.{Date, Locale}
import org.apache.commons.lang3.time.FastDateFormat
/**
  * 日期时间解析工具类
  * 注意：SimpleDateFormat是线程不安全
  */
object DateUtils {

  //注意年份日期时间的格式一定要正确的大小写

  //输入文件日期时间格式：10/Nov/2016:00:01:02 +0800
  //val YYYYMMDDHHMM_TIME_FORMAT = new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss Z", Locale.ENGLISH)
  val YYYYMMDDHHMM_TIME_FORMAT = FastDateFormat.getInstance("dd/MMM/yyyy:HH:mm:ss Z", Locale.ENGLISH)

  //目标日期格式
  //yyyy-MM-dd HH:mm:ss
  //val TARGET_FORMAT = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
  val TARGET_FORMAT = FastDateFormat.getInstance("yyyy-MM-dd HH:mm:ss")
    
  /**
    * 获取时间：yyyy-MM-dd HH:mm:ss
    *
    * @param time
    */
  def parse(time: String) = {
    TARGET_FORMAT.format(new Date(getTime(time)))
  }

  /**
    * 获取输入日志时间：long类型
    *
    * @param str [10/Nov/2016:00:01:02 +0800]
    * @return
    */
  def getTime(time: String) = {
    try {
      YYYYMMDDHHMM_TIME_FORMAT.parse(time.substring(time.indexOf("[") + 1, time.lastIndexOf("]"))).getTime
    }
    catch {
      case e: Exception => {
        0l
      }
    }
  }

  def main(args: Array[String]): Unit = {
    println(parse("[10/Nov/2016:00:01:02 +0800]"))
  }
}
```



## 7 数据清洗之二清洗概述

- 使用Spark SQL解析访问日志
- 解析出课程编号、类型
- 根据IP解析出城市信息
- 使用Spark SQL将访问时间按天进行分区输出



一般的日志处理方式，我们是需要进行分区的，
按照日志中的访问时间进行相应的分区，比如：d（天）,h（时）,m5(每5分钟一个分区)



**输入：**访问时间、访问URL、耗费的流量、访问IP地址信息
**输出：**URL、cmsType(video/article)、cmsId(编号)、流量、ip、城市信息、访问时间、天

**SparkStatCleanJob.scala**

```scala
package com.imooc.log
import org.apache.spark.sql.SparkSession
/**
  * 使用Spark完成我们的数据清洗操作
  */
object SparkStatCleanJob {

  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder().appName("SparkStatCleanJob").master("local[2]").getOrCreate()

    //输入：src/main/data/log/access.log
    val accessRDD = spark.sparkContext.textFile("src/main/data/log/access.log")
    //accessRDD.take(10).foreach(println)
    //将RDD => DataFrame
    val accessDF = spark.createDataFrame(accessRDD.map(x => AccessConvertUtil.parseLog(x)), AccessConvertUtil.struct)
    accessDF.printSchema()
    accessDF.show(false)
    spark.stop()
  }
}
```

**工具类：AccessConvertUtil**

```scala
package com.imooc.log

import org.apache.spark.sql.Row
import org.apache.spark.sql.types.{LongType, StringType, StructField, StructType}

/**
  * 访问日志转换（输入-->输出）工具类
  */
object AccessConvertUtil {

  //定义输出的字段
  val struct = StructType(
    Array(
      StructField("url", StringType),
      StructField("cmsType", StringType),
      StructField("cmsId", LongType),
      StructField("traffic", LongType),
      StructField("ip", StringType),
      StructField("city", StringType),
      StructField("time", StringType),
      StructField("day", StringType)
    )
  )


  /**
    * 根据输入的每一行信息转换成输出的样式
    *
    * @param log 输出的每一行记录信息
    */
  def parseLog(log: String) = {


    try {
      val splits = log.split("\t")

      val url = splits(1)
      val traffic = splits(2).toLong
      val ip = splits(3)

      val domain = "http://www.imooc.com/"
      val cms = url.substring(url.indexOf(domain) + domain.length)
      val cmsTyped = cms.split("/")

      var cmsType = ""
      var cmsId = 0l
      if (cmsTyped.length > 1) {
        cmsType = cmsTyped(0)
        cmsId = cmsTyped(1).toLong
      }

      var city = ""
      val time = splits(0)
      val day = time.substring(0, 10).replaceAll("-", "")

      //这个row里面的字段要和struct中的字段对应上
      Row(url, cmsType, cmsId, traffic, ip, city, time, day)
    }
    catch {
      case e: Exception => Row(0)
    }
  }
}
```



## 9 数据清洗之ip地址解析

根据ip解析城市地址，使用github开源工程[ipdatabase](https://github.com/wzhe06/ipdatabase)

使用github上已有的开源项目

```shell
1）git clone https://github.com/wzhe06/ipdatabase.git
2）编译下载的项目：mvn clean package -DskipTests
3）安装jar包到自己的maven仓库
mvn install:install-file -Dfile=ipdatabase-1.0-SNAPSHOT.jar -DgroupId=com.ggstar -DartifactId=ipdatabase -Dversion=1.0 -Dpackaging=jar

java.io.FileNotFoundException: 
file:/Users/rocky/maven_repos/com/ggstar/ipdatabase/1.0/ipdatabase-1.0.jar!/ipRegion.xlsx (No such file or directory)
添加文件：ipRegion.xlsx   ipDatabase.csv
```

```scala
package com.imooc.log
import com.ggstar.util.ip.IpHelper
/**
  * IP解析工具类
  */
object IpUtils {
  def getCity(ip: String) = {
    IpHelper.findRegionByIp(ip)
  }
  def main(args: Array[String]): Unit = {
    println(getCity("218.75.35.226"))
  }
}
```





## 10 数据清洗存储在目标地址

```scala
val spark = SparkSession.builder().appName("SparkStatCleanJob")
.config("spark.sql.sources.partitionColumnTypeInference.enabled", false)
.master("local[2]").getOrCreate()

//输入：src/main/data/log/access.log
val accessRDD = spark.sparkContext.textFile("src/main/data/log/access.log")

//accessRDD.take(10).foreach(println)

//将RDD => DataFrame
val accessDF = spark.createDataFrame(accessRDD.map(x => AccessConvertUtil.parseLog(x)), AccessConvertUtil.struct)

//accessDF.printSchema()
//accessDF.show(false)
    
//存储数据  控制文件输出的大小：coalesce(int num) 覆写：mode(SaveMode.Overwrite)
accessDF.coalesce(1).write.format("parquet").partitionBy("day").mode(SaveMode.Overwrite).save("src/main/data/log/clean")

spark.stop()
```

**调优点：**
1) 控制文件输出的大小： coalesce(int num)

## 11 需求一：统计功能实现

按照需求完成统计信息并将统计结果入库

- 使用DataFrame API完成统计分析
- 使用SQL API完成统计分析
- 将统计分析结果写入到MySQL数据库





## 12 Scala操作MySql工具类开发

添加Mysql驱动依赖

```xml
<dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>5.1.38</version>
</dependency>
```

mysql 创建 用户 并赋权 远程登陆

```shell
#登陆mysql后，授权如下：
$ GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' WITH GRANT OPTION;

##########################################################
命令如下：

mysql -u root p (这里注意不要输入分号；)

然后再输入密码

这样root用户就登陆成功。

然后再一次输入下面的命令
CREATE USER 'boy'@'%' IDENTIFIED BY '123456';
GRANT ALL PRIVILEGES ON *.* TO 'boy'@'%' WITH GRANT OPTION;
```

```scala
package com.imooc.log
import java.sql.{Connection, DriverManager, PreparedStatement}
/**
  * MySQL操作工具类
  */
object MySQLUtils {

  //获取数据库连接
  def getConnection() = {
    DriverManager.getConnection("jdbc:mysql://192.168.95.128:3306/imooc_project?user=root&password=root")
  }

  //释放数据库等连接资源
  def release(connection: Connection, pstmt: PreparedStatement): Unit = {
    try {
      if (pstmt != null) {
        pstmt.close()
      }
    }
    catch {
      case e: Exception => e.printStackTrace()
    }
    finally {
      if (connection != null) {
        connection.close()
      }
    }
  }


  def main(args: Array[String]): Unit = {
    println(getConnection())
  }
}
```



## 13 需求一 统计结果写入到MySQL

需求一：统计**imooc主站**最受欢迎的课程/手记的Top N访问次数

//统计每一天课程的访问次数的TopN

```shell
表：day_video_access_topn_stat
	天
	课程编号
	访问次数
create table day_video_access_topn_stat (
day varchar(8) not null,
cms_id bigint(10) not null, 
times bigint(10) not null,  
primary key (day, cms_id)
);
```

**3) 批量插入数据库数据，提交使用batch操作**

```
package com.imooc.log

import java.sql.{Connection, PreparedStatement}
import scala.collection.mutable.ListBuffer
/**
  * 各个维度统计的DAO操作
  */
object StatDAO {
  /**
    * 批量保存DayVideoAccessStat到数据库
    *
    * @param list
    */
  def insertDayVideoAccessTopN(list: ListBuffer[DayVideoAccessStat]): Unit = {
    var connection: Connection = null
    var pstmt: PreparedStatement = null
    try {

      connection = MySQLUtils.getConnection()

      //使用批处理 设置手动提交
      connection.setAutoCommit(false)

      val sql = "insert into day_video_access_topn_stat(day,cms_id,times) values(?,?,?)"

      pstmt = connection.prepareStatement(sql)

      for (ele <- list) {
        pstmt.setString(1, ele.day)
        pstmt.setLong(2, ele.cmsId)
        pstmt.setLong(3, ele.times)
        pstmt.addBatch()
      }
      //执行批量处理
      pstmt.executeBatch()
      //手动提交
      connection.commit()
    }
    catch {
      case e: Exception => e.printStackTrace()
    }
    finally {
    }
  }
}
```

统计结果：

```shell
mysql> select * from day_video_access_topn_stat;
+----------+--------+--------+
| day      | cms_id | times  |
+----------+--------+--------+
| 20170511 |  14322 |  55102 |
| 20170511 |   4500 |  55366 |
| 20170511 |   4600 |  55501 |
| 20170511 |  14623 |  55621 |
| 20170511 |  14390 |  55683 |
| 20170511 |  14704 |  55701 |
| 20170511 |   4000 |  55734 |
| 20170511 |  14540 | 111027 |
+----------+--------+--------+
8 rows in set (0.00 sec)
```



## 14 需求二统计功能实现

需求二：按**地市统计**imooc主站最受欢迎的Top N课程

1）根据IP地址提取出城市信息

2）窗口函数在Spark SQL中的使用



## 15 需求二统计结果写入到MySQL

表：day_video_city_access_topn_stat

```shell
create table day_video_city_access_topn_stat (
day varchar(8) not null,
cms_id bigint(10) not null,
city varchar(20) not null,
times bigint(10) not null,
times_rank int not null,
primary key (day, cms_id, city)
);
```

```shell
mysql> select * from day_video_city_access_topn_stat order by city,times_rank asc;
+----------+--------+-----------+-------+------------+
| day      | cms_id | city      | times | times_rank |
+----------+--------+-----------+-------+------------+
| 20170511 |  14540 | 上海市    | 22058 |          1 |
| 20170511 |  14704 | 上海市    | 11219 |          2 |
| 20170511 |   4000 | 上海市    | 11182 |          3 |
| 20170511 |  14540 | 北京市    | 22270 |          1 |
| 20170511 |   4600 | 北京市    | 11271 |          2 |
| 20170511 |  14390 | 北京市    | 11175 |          3 |
| 20170511 |  14540 | 安徽省    | 22149 |          1 |
| 20170511 |  14390 | 安徽省    | 11229 |          2 |
| 20170511 |  14704 | 安徽省    | 11162 |          3 |
| 20170511 |  14540 | 广东省    | 22115 |          1 |
| 20170511 |  14623 | 广东省    | 11226 |          2 |
| 20170511 |  14704 | 广东省    | 11216 |          3 |
| 20170511 |  14540 | 浙江省    | 22435 |          1 |
| 20170511 |  14322 | 浙江省    | 11151 |          2 |
| 20170511 |  14390 | 浙江省    | 11110 |          3 |
+----------+--------+-----------+-------+------------+
15 rows in set (0.00 sec)
```



## 16 需求三统计功能实现

需求三：按**流量统计**imooc主站最受欢迎的TonN课程

表：day_video_traffics_topn_stat

```shell
create table day_video_traffics_topn_stat (
day varchar(8) not null,
cms_id bigint(10) not null,
traffics bigint(20) not null,
primary key (day, cms_id)
);
```

```
def videoTrafficsTopNStat(spark: SparkSession, accessDF: DataFrame, day: String): Unit = {
  import spark.implicits._
  val cityAccessTopNDF = accessDF.filter($"day" === day && $"cmsType" === "video")
    .groupBy("day", "cmsId")
    .agg(sum("traffic").as("traffics"))
    .orderBy($"traffics".desc)
  //.show(false)

  /**
    * 将统计结果写入到MySQL中
    */
  try {
    cityAccessTopNDF.foreachPartition(partitionOfRecords => {
      val list = new ListBuffer[DayVideoTrafficsStat]

      partitionOfRecords.foreach(info => {
        val day = info.getAs[String]("day")
        val cmsId = info.getAs[Long]("cmsId")
        val traffics = info.getAs[Long]("traffics")
        list.append(DayVideoTrafficsStat(day, cmsId, traffics))
      })
      StatDAO.insertDayVideoTrafficsAccessTopN(list)
    })
  }
  catch {
    case e: Exception => e.printStackTrace()
  }
}

case class DayVideoTrafficsStat(day: String, cmsId: Long, traffics: Long)

def insertDayVideoTrafficsAccessTopN(list: ListBuffer[DayVideoTrafficsStat]): Unit = {

    var connection: Connection = null
    var pstmt: PreparedStatement = null

    try {

      connection = MySQLUtils.getConnection()

      //使用批处理 设置手动提交
      connection.setAutoCommit(false)

      val sql = "insert into day_video_traffics_topn_stat(day,cms_id,traffics) values(?,?,?)"

      pstmt = connection.prepareStatement(sql)

      for (ele <- list) {
        pstmt.setString(1, ele.day)
        pstmt.setLong(2, ele.cmsId)
        pstmt.setLong(3, ele.traffics)
        pstmt.addBatch()
      }
      //执行批量处理
      pstmt.executeBatch()
      //手动提交
      connection.commit()
    }
    catch {
      case e: Exception => e.printStackTrace()
    }
    finally {
    }
  }
  
  
```



## 17 需求三统计结果写入到MySQL

统计结果：

```shell
mysql> select * from day_video_traffics_topn_stat;
+----------+--------+----------+
| day      | cms_id | traffics |
+----------+--------+----------+
| 20170511 |  14540 | 55454898 |
| 20170511 |   4500 | 27877433 |
| 20170511 |  14390 | 27895139 |
| 20170511 |  14623 | 27822312 |
| 20170511 |   4000 | 27847261 |
| 20170511 |   4600 | 27777838 |
| 20170511 |  14704 | 27737876 |
| 20170511 |  14322 | 27592386 |
+----------+--------+----------+
8 rows in set (0.00 sec)
```



##  18 代码重构之删除指定日期已有的数据

先删除三个表，然后依次添加到MySql中



**调优点：**
1) 控制文件输出的大小： coalesce(int num)
2) 分区字段的数据类型调整：spark.sql.sources.partitionColumnTypeInference.enabled

> http://spark.apache.org/docs/latest/sql-programming-guide.html#partition-discovery
>
> Notice that the data types of the partitioning columns are automatically inferred. Currently, numeric data types, date, timestamp and string type are supported. Sometimes users may not want to automatically infer the data types of the partitioning columns. For these use cases, the automatic type inference can be configured by `spark.sql.sources.partitionColumnTypeInference.enabled`, which is default to `true`. When type inference is disabled, string type will be used for the partitioning columns. 

3) 批量插入数据库数据，提交使用batch操作



------

Boy-20180605